\documentclass[twoside,11pt]{article}

%================================ PREAMBLE ==================================

%--------- Packages -----------
\usepackage{fullpage}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{latexsym}
\usepackage{graphicx}
\usepackage{wrapfig}
\usepackage{color}
\usepackage{comment}
\usepackage{mathtools}
\usepackage[hyphens]{url}
\usepackage{hyperref}
%\usepackage{algorithm,algorithmic}
\graphicspath{ {images/} }

%---------- Spacing ----------
\setlength{\parindent}{0pt}
\setlength{\parskip}{8pt}

%---------Definitions----------
\newcommand{\half}{{\textstyle{\frac{1}{2}}}}
\renewcommand{\>}{{\rightarrow}}
\renewcommand{\hat}{\widehat}
\renewcommand{\tilde}{\widetilde}
\newcommand{\grad}{{\nabla}}
%
\newcommand{\argmax}{\textup{\textrm{argmax}}}
\newcommand{\argmin}{\textup{\textrm{argmin}}}
\newcommand{\argsort}{\textup{\textrm{argsort}}}
\newcommand{\sign}{\textup{\textrm{sign}}}
\newcommand{\poly}{\textup{\textrm{poly}}}
\newcommand{\er}{\textup{\textrm{er}}}
\newcommand{\zo}{\textup{\textrm{0-1}}}
\newcommand{\sq}{\textup{\textrm{sq}}}
%
\newcommand{\1}{{\mathbf 1}}
\newcommand{\0}{{\mathbf 0}}
\newcommand{\I}{{\mathbf I}}
\newcommand{\R}{{\mathbb R}}
\newcommand{\Z}{{\mathbb Z}}
\newcommand{\N}{{\mathbb N}}
\renewcommand{\P}{{\mathbf P}}
\newcommand{\E}{{\mathbf E}}
\newcommand{\Var}{{\mathbf{Var}}}
%
\renewcommand{\a}{{\mathbf a}}
\renewcommand{\b}{{\mathbf b}}
\renewcommand{\c}{{\mathbf c}}
\renewcommand{\d}{{\mathbf d}}
\newcommand{\f}{{\mathbf f}}
\renewcommand{\k}{{\mathbf k}}
\newcommand{\p}{{\mathbf p}}
\newcommand{\q}{{\mathbf q}}
\renewcommand{\u}{{\mathbf u}}
\newcommand{\w}{{\mathbf w}}
\newcommand{\x}{{\mathbf x}}
\newcommand{\y}{{\mathbf y}}
%
\newcommand{\A}{{\mathbf A}}
\newcommand{\bC}{{\mathbf C}}
\newcommand{\C}{{\mathcal C}}
\newcommand{\cD}{{\mathcal D}}
\newcommand{\F}{{\mathcal F}}
\renewcommand{\H}{{\mathcal H}}
\newcommand{\K}{{\mathbf K}}
\renewcommand{\L}{{\mathcal L}}
\newcommand{\bL}{{\mathbf L}}
\newcommand{\cN}{{\mathcal N}}
\newcommand{\W}{{\mathbf W}}
\newcommand{\X}{{\mathcal X}}
\newcommand{\bX}{{\mathbf X}}
\newcommand{\Y}{{\mathcal Y}}
%
\newcommand{\bloss}{{\boldsymbol \ell}}
\newcommand{\blambda}{{\boldsymbol \lambda}}
\newcommand{\bmu}{{\boldsymbol \mu}}
\newcommand{\bnu}{{\boldsymbol \nu}}
\newcommand{\bSigma}{{\boldsymbol \Sigma}}
\newcommand{\seta}{{\boldsymbol \eta}}
\newcommand{\bpsi}{{\boldsymbol \psi}}
\newcommand{\bphi}{{\boldsymbol \phi}}
\newcommand{\bPhi}{{\boldsymbol \Phi}}
\newcommand{\balpha}{{\boldsymbol \alpha}}
\newcommand{\bxi}{{\boldsymbol \xi}}

%=============================== END PREAMBLE ===============================

\begin{document}

%================================ COVER PAGE ================================

\emph{\footnotesize{CIS 520 Spring 2018, Project Report}}

\vspace{12pt}

%Fill in your project title
\textbf{\Large{Information Extraction from Structured and Unstructured Documents with Machine Learning}}

\vspace{1cm}

\textbf{Team Members:}

%Fill in your team details; remove any lines that are not needed
Joseph Cappadona (PennKey: \texttt{jcapp}; Email: \texttt{jcapp@seas.upenn.edu}) \\

%---

\vspace{2cm}

\textbf{Assigned Project Mentor:}

%Fill in assigned TA name
Anant Maheshwari

\vspace{1cm}

\textbf{Team Member Contributions:}

%Fill in team member contributions
\begin{center}
\begin{tabular}{|l|l|}
\hline
Team Member & Contributions \\
\hline
Joseph Cappadona & * \\
\hline
\end{tabular}
\end{center}

\vspace{12pt}

\textbf{Code Submission:}

Code can be found here: \url{https://github.com/josephcappadona/cis520-final-project}

\newpage
%============================= MAIN DOCUMENT ================================

\begin{abstract}
Information extraction is important to any data pipeline. But automating information extraction can be difficult, and there are many instances where manual data collection and entry is the only option. A robust document recognition and information extraction engine would vastly improve the rate at which certain types of data could be collected. We have taken inspiration from a variety of previous works on structured and unstructured information extraction in an effort to build a machine learning model that can handle a wide variety of document types and data formats. We combine a traditional bag-of-words model with textual clustering and alignment and layout analysis to generate a large number of document features. We found the best performance with tree-based classifiers, achieving, for the Ghega patent data set and Ghega data-sheet data set, generalization errors of 6.41\% (Random Forest, n=31, max\_depth=26) and 3.23\% (Decision Tree, max\_depth=28), respectively.
\end{abstract}

% ==========================================================================================

\section{Introduction}
Information extraction via document classification and recognition can be used to extract data from a wide variety of document types, such as from invoices \cite{LiuWanZhang2016}, memos, and advertisements; from Heads-Up Diplays (HUDs), like those used in mission simulators \cite{Guarino2013} and on sports broadcasts; and from web pages \cite{Gogar2016}. Due to the unstructured nature of most document types, information extraction algorithms need to consider a large number of both text-level features (words, data types) and document-level features (relative positioning and alignment of text). The goal in this paper is to draw from past research into structured and unstructued information extraction to develop a robust set of features which allows for high quality, generalizable information extraction.

% ==========================================================================================

\section{Related Work}
In \cite{Guarino2013}, the authors built an image processing application with pattern recognition to analyze the HUD of Flight Test Campaigns and extract information important to the analysis of the campaign, such as aircraft position parameters, aircraft configuration information, tracking mode, and time. Major limitations included its naive text recognition system, and its poor image processing efficiency and text recognition accuracy, which were both largely due to the poor quality of the extracted HUD images. Additionally, this type of analysis is only effective for static, structured documents.

\cite{Thakur2012} approached information extraction from unstructured documents by utilizing automated context-free grammar learning and alignment-based learning. This type of grammar learning provides a dynamic, adaptive approach to information extraction that is necessary for schema learning. Similarly, \cite{Dejean2015} implemented layout analysis in order to learn common layout schemas for unstructured documents.

In \cite{LiuWanZhang2016}, the authors built an unstructured document recognition system for business invoice processing centered around a bag-of-words approach that attempts to capture common layout and text features. They experimented with Naive Bayes, Multiclass (One-vs-All) Logistic Regression, and Multiclass (One-vs-All) Linear SVM. They were able to achieve 8.81\% training error and 13.99\% test error (through SVM) over a data set of 97 raw invoice images obtained from the internal testing library of the Oracle Corporation. They found that in all situations $L2$ regularization outperformed $L1$ regularization.

Comprehensive surveys such as \cite{Kumar2017, Nguyen2017} demonstrate the potential for the use of deep neural networks for information extraction and document classification. However, these approaches rely on large repositories of documents, which can be problematic for certain use cases.

% ==========================================================================================

\section{Data Set}
The primary data set utilized was the Ghega-dataset, "a dataset for document understanding and classification" (\url{http://machinelearning.inginf.units.it/data-and-tools/ghega-dataset}). This data set consists of 110 data-sheets of electronic components in English and 136 patents in 7 different languages. For each document, a pre-processed, 300 DPI image is provided, along with a blocks CSV and a groundtruth CSV. The blocks CSV contains all text blocks detected via OCR. The groundtruth CSV contains only the text blocks in the blocks file that correspond to desired pieces of information and the associated labels (if they exist). See Appendices A and B for examples of each document.

Many other types of data sets would have been useful to test with, especially those described in the Introduction, however finding labeled data sets for these applications is difficult. With more time, we would have hand-labeled small sets of HUD data, advertisement data, and web page data to test on.

% ==========================================================================================

\section{Problem Formulation}
Given a set of documents $X$ with associated sets of labels $Y$, our goal is to train a classifier $h$ to, given a new document instance $x$, pull out features of interest $f_i$ and classify them into categories $\hat{c_i}$. Accordingly, we will attempt to minimize the loss

\[ L = \sum_{f_i \in x}{\mathbf{1}(h(f_i), c_i)} \]

or, in other words, a loss of 1 is incurred for each feature $f_i$ which is misclassified. Since feature generation and selection is deterministic, we can be assured that we will generate the same features $f_i$ from each $x$ on each pass over the data.

% ==========================================================================================

\section{Algorithms}
\textbf{Text Extraction:} Although feature and label data were provided in the Ghega-dataset, text extraction was implemented using \text{pytesseract}, a Python wrapper for the Tesseract Optical Character Recognition (OCR) engine.

\textbf{Text Cluster Formation:} In order to provide a more robust document model, extracted text was clustered using Density-Based Spatial Cluster of Applications with Noise (DBSCAN). This allows for the generation of more complex feature sets than would otherwise be possible. See Appendices A and B for examples of the results of DBSCAN text clustering.

\textbf{Feature Generation:} Feature generation consisted of analyzing the content in blocks as well as relative positioning and content between blocks. Each text block was first cleaned by replacing particular data types with generic representations ("MONEY", "DATE", "TELE", "EMAIL", "NUM", "ALPHANUM"). Next, for each text block, the following features were generated: \texttt{num\_words}, \texttt{num\_chars}, \texttt{is\_text}, \texttt{is\_numeric}, \texttt{is\_alphanumeric}, \texttt{cluster}, \texttt{x}, \texttt{y}, \texttt{w}, \texttt{h}, \texttt{hAlign\_WORD}, \texttt{vAlign\_WORD}, \texttt{sameCluster\_WORD}, \texttt{vecTo\_WORD\_x}, \texttt{vecTo\_WORD\_y}. Most of these features are self-explanatory, but \texttt{cluster} corresponds to the document cluster (generated with k-Means cluster, k=50) that the text block is closest to, and for the features with \texttt{WORD} in them, \texttt{WORD} was replaced with each word in the block's text cluster (the text cluster generated using DBSCAN) for which the feature is true. That is, \texttt{vAlign\_WORD} (\texttt{hAlign\_WORD}) is true for each word that a text block is vertically (horizontally) aligned with. \texttt{sameCluster} is true for each word that a text block shares a text cluster with. And \texttt{vecTo\_WORD\_x} and \texttt{vecTo\_WORD\_y} correspond to the $x$ and $y$ components of the vector pointing from the text block to each word that is in the same text cluster. Additionally, in order to minimize the number of features generated, we only generated a feature corresponding to \texttt{WORD} if that word was in the 750 most common words throughout the data. The goal of these features, particularly the features related to text clusters, was to better capture spatial features than a bag-of-words approach with elementary layout analysis. This approach resulted in approximately 4000 features for both data sets.

\textbf{Feature Selection:} To further narrow down the number of features, the relative entropy between the feature and label distributions was computed. In particular, the Kullback-Leibler (KL) Divergence was computed for the distributions $p(x_i)p(y)$ and $p(x_i, y)$, for each feature $x_i$. The 2000 highest scoring features were kept.

\textbf{Learning Algorithms:} Several machine learning algorithms were applied to the data. The algorithms that yielded particularly promising results were Decision Trees (and other tree-based classifiers), Logistic Regression, and SVM.

% ==========================================================================================

\section{Experimental Design and Results}
For Ghega-dataset Patents:
\begin{center}
    \begin{tabular}{|l|c|c|c|}
    \hline
    \textbf{Algorithm}     & \textbf{err\_train (\%)} & \textbf{err\_test (\%)} & \textbf{Hyperparameter} \\
    \hline
    Decision Tree          & 3.04   & 9.03    & d=26  \\
    \hline
    Bagging                & 0.05   & 7.09    & DT \\
    \hline
    Random Forest          & 0.04   & 6.41    & n=31  \\
    \hline
    Logistic Regression    & 4.41   & 18.07   & L1    \\
    \hline
    SVM                    & 1.77   & 21.46   & -   \\
    \hline  
    kNN                    & 13.37   & 19.71   & k=5   \\
    \hline  
    \end{tabular}
\end{center}

The tree-based classifiers (Decision Tree, Bagging-DT, Random Forest) yielded the lowest generalization error by far, with the lowest being Random Forest's $6.41\%$. Logistic Regression, SVM, and k-Nearest Neighbors did not generalize well, as shown by the generalization errors of $18.07\%$, $21.46\%$, and $19.71\%$, respectively. \\

For Ghega-dataset Data-sheets:
\begin{center}
    \centering
    \begin{tabular}{|l|c|c|c|}
    \hline
    \textbf{Algorithm}     & \textbf{err\_train (\%)} & \textbf{err\_test (\%)} & \textbf{Hyperparameter} \\
    \hline
    Decision Tree          & 0.52   & 3.23    & d=28  \\
    \hline
    Bagging                & 0.10   & 3.70    & DT \\
    \hline
    Random Forest          & 0.07   & 3.57    & n=31  \\
    \hline
    Logistic Regression    & 1.92   & 9.74   & L1    \\
    \hline
    SVM                    & 0.20   & 28.90   & -   \\
    \hline
    kNN                    & 6.04   & 7.87   & k=5   \\
    \hline  
    \end{tabular}
\end{center}

Similarly to the patent data, the tree-based classifiers yielded the lowest generalization errors, with a depth-28 Decision Tree yielding the lowest generalization error with 3.23\%. Also similar to the patent data, Logistic Regression and SVM did not generalize well. Unlike the patent data, however, k-Nearest Neighbors classifiers did surprisingly well with only 7.87\% generalization error.

% ==========================================================================================

\section{Conclusion and Discussion}
For the both data sets, the tree-based classifiers substantially outperformed all other classifiers. We think that this is most likely due to the fact that the features generated were designed with the idea in mind that most documents, especially those considered here, are tree-structured, with text blocks and text clusters related to one another through parent-child and child-child relationships. Given the large number of features, it is unsurprising that Logistic Regression and SVM did not generalize well. Unlike in \cite{LiuWanZhang2016}, we found that $L1$-regularization outperformed $L2$-regularization substantially. This provides evidence that the regularization technique that will perform best is dependent on the document being modeled.

Future work should involve conducting similar analyses on new data sets. Information extraction can be applied to a wide variety of documents, and as this study shows, different classifiers will perform differently on different types of documents. While tree-based classifiers worked well for the data analyzed in this paper, we hypothesize that the more structured a document is, the less complex the features need to be, and less complex classifiers will perform better.

\newpage
%============================= BIBLIOGRAPHY ===============================

\bibliographystyle{unsrt}
\bibliography{final-paper}

\newpage

\section{Appendices}
\subsection{Appendix A: Ghega Patent Data Example}
\includegraphics[width=10cm]{patent}

\includegraphics[width=8cm]{key}

\newpage
\subsection{Appendix B: Ghega Data-Sheet Data Example}
\includegraphics[width=10cm]{datasheet}

\includegraphics[width=8cm]{key}

\end{document}

%=========================== END DOCUMENT ==============================

